---
title: 'CAMP Project: QA/QC Data Analysis'
author: "Md Ismail Hossain"
date: '2023-07-03'
output: pdf_document
---

```{r setup, message = FALSE, warning = FALSE, include = FALSE}
knitr::opts_chunk$set(echo = FALSE,message = FALSE, warning = FALSE)

```

\newpage

```{r, message = FALSE, warning = FALSE, include = FALSE}

# Main data
library(readr)
df <- read_csv("E:/CAMP/Clean_QC_Data/Camp_PhaseII_Production_06192023_03-Jul-23_11-24/data/Culvert_Collection_Points_2023A.csv")




# QC data

qc_df <-  read_csv("E:/CAMP/Clean_QC_Data/qc_df_updated_1.csv")


library(stringr)

# Modify the "Question" column
qc_df$Question <- str_replace_all(qc_df$Question, "\\s+", "")  # Remove spaces
qc_df$Question <- str_to_lower(qc_df$Question)  # Convert to lowercase
qc_df$Question <- str_remove_all(qc_df$Question, "[^a-z]")  # Keep only text (remove non-alphabetic characters)

library(tidyverse)

# Reshape the data from long to wide format
wide_df_qc <- qc_df %>%
  pivot_wider(names_from = Question,
              values_from = c(Input, Comment_QC, Comment_FO),
              names_sep = "_") %>%
  unnest(everything())

wide_df_qc$Team <- substr(wide_df_qc$`Culvert ID`, 1, 1)


wide_df_qc$filter_good_id <- rowSums(is.na(wide_df_qc))
wide_df_qc$filter_good_id <- ifelse(wide_df_qc$filter_good_id == 180,"No_issue","At_least_one_issue")

```


```{r,echo=FALSE,message = FALSE, warning = FALSE}
library(lubridate)
# Convert the CreationDateTime column to a DateTime object
df$CreationDateTime <- ymd_hms(df$CreationDateTime)

# Create separate Date and Time columns
df$Date <- as.Date(df$CreationDateTime)
df$Time <- format(df$CreationDateTime, "%H:%M:%S")


# Calculate the start and end dates
start_date <- min(df$Date)
end_date <- max(df$Date)

# Create text strings
start_text <- paste(as.character(start_date))
end_text <- paste(as.character(end_date))
```
# Exploratory Analysis of Main Data:


Data collection starting from `r start_text` and we are using the data till `r end_text`.

```{r,echo=FALSE,message = FALSE, warning = FALSE}
# Create a new column for the team identification
df$Team <- substr(df$CulvertID, 1, 1)
paste("Number of Rows:", as.character(dim(df))[1])


# Get the overall unique count of CulvertID
overall_count <- length(unique(df$CulvertID))

paste("Number of unique culverts:", as.character(overall_count))

```
So, there are `r dim(df)[1]` rows in the data and `r overall_count` unique culvert inspected. Let's find the culverts which are not uniquely entered. 

```{r,echo=FALSE}
# Find the duplicate CulvertID values
duplicate_culvertID <- df[duplicated(df$CulvertID) | duplicated(df$CulvertID, fromLast = TRUE), "CulvertID"]

# Print the duplicate CulvertID values
print(unique(duplicate_culvertID))

```
So, this 8 culvert have entered more than one times. We need to fix the entry for them.


Let's find the number of culvert inspected by team:
```{r}
# Load the required packages
library(dplyr)
library(ggplot2)

# Calculate the count of culverts inspected by each team
team_counts <- df %>%
  group_by(Team) %>%
  summarize(CulvertCount = n())
team_counts


```

Let's find the rows where the Team ID have discrepancies.

```{r}
# Find the rows where Team ID is other than A, B, C, D, and E
TeamID_issues <- df[!df$Team %in% c("A", "B", "C", "D", "E"), ]
TeamID_issues[,c("CulvertID","Collector")]
```

Let's remove this 

```{r}
# Calculate the count of culverts inspected by each team
filtered_df <- df[df$Team %in% c("A", "B", "C", "D", "E"), ]


team_counts <- filtered_df %>%
  group_by(Team) %>%
  summarize(CulvertCount = n())


# Create a bar chart using ggplot2
ggplot(data = team_counts, aes(x = Team, y = CulvertCount)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_text(aes(label = CulvertCount), vjust = -0.5) +  # Add value labels
  labs(x = "Team", y = "Count of Culverts Inspected") +
  ggtitle("Number of Culverts Inspected by Each Team")
```
\newpage
\textbf{GPS coordinate location accuracy:}

```{r}
df$Location_accuracy <- ifelse(df$HorizEstAcc > 1.5, "Inaccurate_Location(>1.5)","Accurate_Location") 


# Create the table
location_table <- table(df$Location_accuracy)

# Create a data frame from the table
location_df <- data.frame(Location_accuracy = names(location_table),
                          Count = as.numeric(location_table))

# Calculate the percentage
location_df$Percentage <- location_df$Count / sum(location_df$Count) * 100

# Create the pie chart using ggplot2
pie_chart <- ggplot(data = location_df, aes(x = "", y = Count, fill = Location_accuracy)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y", start = 0) +
  labs(fill = "Location Accuracy", x = NULL, y = NULL, title = "Location Accuracy Distribution (HorizEstAcc)") +
  theme_void() +
  theme(legend.position = "right") +
  geom_text(aes(label = paste0(round(Percentage), "%")), position = position_stack(vjust = 0.5))

# Display the pie chart
print(pie_chart)
```

Now we have a hypothesis that if there is an error occurred in any column then there is high possibility that there will be another error made on measuring the location accuracy. We will test this hypothesis in the later section.


\newpage

## Exploratory analysis of QC Data:

```{r}
#dim(wide_df_qc)[1]
```
Total QC item is `r dim(wide_df_qc)[1]` among `r overall_count` culvert collected till `r end_text`, which is `r round(dim(wide_df_qc)[1]/overall_count,2)*100`\% overall. Let's observe the team wise inspection and the \% of error (at least one error):

```{r}
total_qc_by_tm <- as.data.frame(table(wide_df_qc$Team))
at_least_one_err <- as.data.frame(table(wide_df_qc$Team,wide_df_qc$filter_good_id))
at_least_one_err <- subset(at_least_one_err, Var2 == 'At_least_one_issue')

# Entering data
Team <- at_least_one_err$Var1
Total_QC <- total_qc_by_tm$Freq
Error <- at_least_one_err$Freq

qc_per_tab <- data.frame(Team,Total_QC,Error)
qc_per_tab$error_per <- round(qc_per_tab$Error/qc_per_tab$Total_QC,2)


# Create the bar plot
library(ggplot2)

ggplot(qc_per_tab, aes(x = Team)) +
  geom_bar(aes(y = Total_QC), stat = "identity", fill = "cyan", colour = "#006000") +
  geom_line(aes(y = error_per * max(Total_QC)), color = "red", size = 1, group = 1) +
  geom_point(aes(y = error_per * max(Total_QC)), color = "red", size = 3) +
  labs(title = "Number of QC Items and Percentage of Occurring Error",
       x = "Team", y = "Total QC Items") +
  scale_y_continuous(
    name = "Total QC Items",
    sec.axis = sec_axis(
      ~ . / 352,
      name = "Percentage of Occurring Error",
      labels = function(x) paste0(x * 100, "%"),
      breaks = scales::pretty_breaks(n = 5)
    )
  ) +
  geom_text(aes(y = Total_QC, label = Total_QC), vjust = -0.3) +
  geom_text(aes(y = error_per * 100, label = paste0(error_per * 100, "%")), vjust = -3) +
  theme_bw()


```

## A closer look on error data:

```{r}
qc_df_with_err <- wide_df_qc %>% filter(filter_good_id == "At_least_one_issue")
```
So, in total `r dim(qc_df_with_err)[1]` culvert id have at least one issue. 






```{r}
# Assuming your dataframe is called 'data'
qc_df_with_err$non_missing_count <- rowSums(!is.na(qc_df_with_err[, 2:181]))
#summary(qc_df_with_err$non_missing_count)

qc_df_with_err$error_count <- ifelse(qc_df_with_err$non_missing_count <= 3, "One_Error", "> One_Error")
table(qc_df_with_err$error_count)

```
When an error is made in more than one entry or exactly in one entry, this tally will indicate the corresponding number of culverts.


```{r}
# Inner join 
colnames(qc_df_with_err)[1] <- 'CulvertID'
df_merged <- merge(qc_df_with_err,df, by = "CulvertID")
mismatched_id <- setdiff(qc_df_with_err$CulvertID, df$CulvertID)

```


After merging the main data with QC data (infected id's only) we are observing some id mismatch. This happens may be that id data removed from the database or something else happened. This are the id's (total `r length(mismatched_id)` culvert id) which are not matching with the main data set although we did the QC for them: 

`r mismatched_id`

\newpage

## Hypothesis testing:

Let's find number of culvert where there is also location error: 

```{r}
table(df_merged$error_count,df_merged$Location_accuracy)
```

Now we have the hypothesis that if an error occurs in any column, there is a high likelihood that another error will occur when measuring the location accuracy. This hypothesis will be tested in a subsequent section.

The above contingency table demonstrates that only 5 of the QC items with the error also have an inaccurate location issue. We can conduct a statistical test of the following hypothesis:

\textbf{$H_0$:} Error counts per culvert and GPS location error are independent. 

\textbf{$H_1$:} Error counts per culvert and GPS location error are dependent.

```{r}
cor.test(df_merged$non_missing_count,df_merged$HorizEstAcc, 
                    method = "pearson")

```

The p-value calculated is greater than 0.05. Therefore, at a 5\% level of significance, we cannot reject the null hypothesis or conclude that error counts per culverts are independent of GPS location (HorizEstAcc) error.

## Error Trend (at least one error per culvertID):

```{r}
library(ggplot2)
library(dplyr)
library(scales)

# Assuming you have a dataframe named 'df_merged' with a column named 'Date'

# Count the occurrences of each date
date_counts <- df_merged %>%
  count(Date)

# Calculate the overall average
overall_average <- mean(date_counts$n)

# Create the time series line chart
ggplot(date_counts, aes(x = Date, y = n)) +
  geom_line(color = "steelblue", size = 1) +
  geom_hline(yintercept = overall_average, color = "red", linetype = "dashed", size = 1) +
  labs(x = "Date", y = "Daily Error Count", title = "Error Line Chart") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_x_date(breaks = date_breaks("1 day"), labels = date_format("%Y-%m-%d"))


```

Daily average count of the error is `r round(overall_average,2)` which presented using red dashed line. Let's observe few analysis (word cloud) using the comments made by Field officer and QA/QC analyst:


## Word cloud of QC comments:

```{r}
# Preprocess the text data
library(tm)
library(wordcloud)

# Assuming you have a dataframe named 'df' and a text column named 'text_column'

# Create a Corpus from the text column
corpus <- Corpus(VectorSource(qc_df$Comment_QC))

# Preprocess the corpus
corpus <- tm_map(corpus, content_transformer(tolower))  # Convert text to lowercase
corpus <- tm_map(corpus, removePunctuation)  # Remove punctuation
corpus <- tm_map(corpus, removeNumbers)  # Remove numbers
corpus <- tm_map(corpus, removeWords, stopwords("english"))  # Remove common English stopwords

# Create a document-term matrix
dtm <- DocumentTermMatrix(corpus)

# Calculate term frequencies
word_freq <- colSums(as.matrix(dtm))

# Generate word cloud
{wordcloud(names(word_freq), freq = word_freq, random.order = FALSE, colors = brewer.pal(8, "Dark2"))

# Adjust word cloud appearance
title( col.main = "black", font.main = 1, cex.main = 1.5)}







```

## Word cloud of FO comments:

```{r}
# Preprocess the text data
library(tm)
library(wordcloud)

# Assuming you have a dataframe named 'df' and a text column named 'text_column'

# Create a Corpus from the text column
corpus <- Corpus(VectorSource(qc_df$Comment_FO))

# Preprocess the corpus
corpus <- tm_map(corpus, content_transformer(tolower))  # Convert text to lowercase
corpus <- tm_map(corpus, removePunctuation)  # Remove punctuation
corpus <- tm_map(corpus, removeNumbers)  # Remove numbers
corpus <- tm_map(corpus, removeWords, stopwords("english"))  # Remove common English stopwords

# Create a document-term matrix
dtm <- DocumentTermMatrix(corpus)

# Calculate term frequencies
word_freq <- colSums(as.matrix(dtm))

# Generate word cloud
{wordcloud(names(word_freq), freq = word_freq, random.order = FALSE, colors = brewer.pal(8, "Dark2"))

# Adjust word cloud appearance
title(col.main = "black", font.main = 1, cex.main = 1.5)}

```










## Word cloud of comments (main data):


```{r}

# Preprocess the text data
library(tm)
library(wordcloud)

# Assuming you have a dataframe named 'df' and a text column named 'text_column'

# Create a Corpus from the text column
corpus <- Corpus(VectorSource(df$Comments))

# Preprocess the corpus
corpus <- tm_map(corpus, content_transformer(tolower))  # Convert text to lowercase
corpus <- tm_map(corpus, removePunctuation)  # Remove punctuation
corpus <- tm_map(corpus, removeNumbers)  # Remove numbers
corpus <- tm_map(corpus, removeWords, stopwords("english"))  # Remove common English stopwords

# Create a document-term matrix
dtm <- DocumentTermMatrix(corpus)

# Calculate term frequencies
word_freq <- colSums(as.matrix(dtm))

# Generate word cloud
{wordcloud(names(word_freq), freq = word_freq, random.order = FALSE, colors = brewer.pal(8, "Dark2"))

# Adjust word cloud appearance
title( col.main = "black", font.main = 1, cex.main = 1.5)}

```





```{r}
# dim(df_merged)
# # Filter out columns with complete missing values
# df_qc_complete_case <- df_merged[, apply(!is.na(df_merged), 2, any)]
# dim(df_qc_complete_case)
# View(df_qc_complete_case)
```


